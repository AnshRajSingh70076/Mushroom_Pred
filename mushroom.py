# -*- coding: utf-8 -*-
"""Mushroom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zealzCQvkSLVaGPrPWHBj_qP5pi3KGxz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.svm import SVC

import warnings
warnings.filterwarnings('ignore')

tdata=pd.read_csv('/content/test.csv')
data=pd.read_csv('/content/train.csv')

tdata.shape,data.shape

for i in data.columns:
  if(((data[i].isnull().sum())/(data.shape[0]))>0.85):
    data.drop(i,axis=1,inplace=True)

for i in tdata.columns:
  if(((tdata[i].isnull().sum())/(tdata.shape[0]))>0.85):
    tdata.drop(i,axis=1,inplace=True)

for col in data.select_dtypes(include=['object']).columns:
    most_frequent = data[col].mode()[0]
    data[col].fillna(most_frequent, inplace=True)


for col in tdata.select_dtypes(include=['object']).columns:
    most_frequent = tdata[col].mode()[0]
    tdata[col].fillna(most_frequent, inplace=True)

skewness_value = data['cap-diameter'].skew()
skewness_value2 = data['cap-diameter'].skew()

median_value = data['cap-diameter'].median()
median_value2 = tdata['cap-diameter'].median()

data['cap-diameter'] = data['cap-diameter'].fillna(median_value)
tdata['cap-diameter'] = tdata['cap-diameter'].fillna(median_value2)

data.columns,tdata.columns

data.isnull().sum(),tdata.isnull().sum()

tdata['stem-height'] = tdata['stem-height'].fillna(tdata['stem-height'].median())
  # data['stem-height'] = data['stem-height'].fillna(data['stem-height'].median())
  # tdata['stem-width'] = tdata['stem-width'].fillna(tdata['stem-width'].median())
  # data['stem-width'] = data['stem-width'].fillna(data['stem-width'].median())

tdata.isnull().sum(),data.isnull().sum()

print(data['class'].unique())

data['class'] = data['class'].replace({'e': 0, 'p': 1})
   data.isnull().sum()

data.head()

# y=data['class']
# y.head()
# data.drop('class',axis=1,inplace=True)

data.shape,tdata.shape
categorical_columns_test = tdata.select_dtypes(include=['object']).columns
categorical_columns_train = data.select_dtypes(include=['object']).columns

from sklearn.preprocessing import OrdinalEncoder

# Initialize an OrdinalEncoder with the ability to handle unseen categories
ordinal_encoders = {}
for col in categorical_columns_train:
    ordinal_encoders[col] = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
    data[col] = ordinal_encoders[col].fit_transform(data[[col]])

# Transform the test data using the same encoders
for col in categorical_columns_test:
    if col in ordinal_encoders:
        tdata[col] = ordinal_encoders[col].transform(tdata[[col]])

data.isnull().sum(),tdata.isnull().sum()



data=data.drop('id',axis=1)
tdata=tdata.drop('id',axis=1)

tdata.head()

data.head()

def remove_outliers_iqr(df):
    df_no_outliers = data.copy()
    for column in df.columns:
        if pd.api.types.is_numeric_dtype(df[column]):

            Q1 = data[column].quantile(0.25)
            Q3 = data[column].quantile(0.75)


            IQR = Q3 - Q1


            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR


            df_no_outliers = df_no_outliers[(df_no_outliers[column] >= lower_bound) & (df_no_outliers[column] <= upper_bound)]

    return df_no_outliers


cdata = remove_outliers_iqr(data)

def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_no_outliers = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df_no_outliers


cldata = cdata.copy()


for column in cldata.columns:
    if pd.api.types.is_numeric_dtype(cldata[column]):
        cldata = remove_outliers_iqr(cldata, column)

X=cdata.drop('class',axis=1)
y=cdata['class']

cdata.isnull().sum()

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
tdata_scaled = scaler.transform(tdata)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_val)


accuracy = accuracy_score(y_val, y_pred)
print(f"Accuracy: {accuracy}")
print("Classification Report:")
# print(classification_report(y_test, y_pred))

y_test = rf_model.predict(tdata_scaled)
print(f"Test predictions: {y_test}")

y_test_decoded = []
for i in y_test:
    if i == 0:
        y_test_decoded.append('e')
    else:
        y_test_decoded.append('p')

# Load the sample submission file to check the format
sample_submission = pd.read_csv('/content/sample_submission.csv')

# Prepare your prediction DataFrame
# Make sure the column names match the sample submission
submission_df = pd.DataFrame({
    'id': sample_submission['id'],  # Use the ID column from your test data
    'prediction': y_test_decoded  # Ensure this matches the column name expected in the sample submission
})

# Align the columns with the sample submission format
submission_df.columns = sample_submission.columns

# Save the DataFrame to a CSV file
submission_df.to_csv('/content/done3.csv', index=False)

print("Submission file created successfully: /content/submission.csv")

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(random_state=42)

# Fit the model to the training data
logreg.fit(X_train,y_train)

# Predict on the validation data
y_pred = logreg.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy:.4f}")

from sklearn.svm import SVC

svc = SVC(kernel='linear', random_state=42)

# Fit the model to the training data
svc.fit(X_train, y_train)

# Predict on the validation data
y_pred = svc.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy:.4f}")

y.shape,X.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Modify the output layer to have 1 unit for binary classification with sigmoid activation
model = Sequential()

# Input layer and hidden layers
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))

# Output layer with 1 unit (for binary classification) and sigmoid activation
model.add(Dense(1, activation='sigmoid'))

# Compile the model using Adam optimizer and binary crossentropy loss
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, batch_size=32)

predictions = model.predict(tdata_scaled)

# If the model has a sigmoid activation in the output layer (binary classification)
# The predictions will be probabilities, so you can convert them to class labels (0 or 1)
predicted_classes = (predictions > 0.5).astype(int)

# Convert predicted_classes to a list
predicted_classes = predicted_classes.flatten().tolist()

# Map 0 to 'e' and 1 to 'p'
for i in range(len(predicted_classes)):
    if predicted_classes[i] == 0:
        predicted_classes[i] = 'e'
    else:
        predicted_classes[i] = 'p'

print(predicted_classes)

sample_submission = pd.read_csv('/content/sample_submission.csv')

# Prepare your prediction DataFrame
# Make sure the column names match the sample submission
submission_df = pd.DataFrame({
    'id': sample_submission['id'],  # Use the ID column from your test data
    'prediction': predicted_classes  # Ensure this matches the column name expected in the sample submission
})

# Align the columns with the sample submission format
submission_df.columns = sample_submission.columns

# Save the DataFrame to a CSV file
submission_df.to_csv('/content/done4.csv', index=False)

print("Submission file created successfully: /content/submission.csv")